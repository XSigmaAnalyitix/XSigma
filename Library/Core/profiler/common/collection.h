#pragma once

#include <cstdint>
#include <memory>
#include <mutex>
#include <type_traits>
#include <utility>
#include <variant>

// TODO: Missing XSigma dependencies - original includes were:
// //#include <XSigma/Context.h>
// #include <xsigma/csrc/utils/python_stub.h>
// These are XSigma-specific headers not available in XSigma
// #include <xsigma/core/TensorImpl.h>
#include "common/macros.h"
#include "memory/device.h"
#include "profiler/base/base.h"
#include "profiler/base/perf.h"
#include "profiler/common/containers.h"
#include "profiler/common/data_flow.h"
#include "profiler/common/events.h"
#include "profiler/common/orchestration/python_tracer.h"
#include "profiler/common/util.h"
#include "profiler/kineto/kineto_shim.h"
#include "util/approximate_clock.h"
#include "util/flat_hash.h"
#include "util/strong_type.h"

// Minimal layout constant expected by profiler code for Tensor::layout()
namespace xsigma
{
inline constexpr int kStrided = 0;
}

namespace xsigma::profiler::impl
{

enum class EventType : uint8_t
{
    TorchOp = 0,
    Backend,
    Vulkan,
    Allocation,
    OutOfMemory,
    PyCall,
    PyCCall,
    Kineto,
    PythonGC
};

// ============================================================================
// == Value (Tensor, Scalar) summary ==========================================
// ============================================================================
struct XSIGMA_VISIBILITY RawTensorMetadataBase
{
    RawTensorMetadataBase() = default;
    explicit RawTensorMetadataBase(const xsigma::Tensor& t);

    StorageImplData data_;
    int             dtype_{0};
    int             layout_{0};
    uint32_t        size_dim_{0};
};

// Collected during profiling.
struct XSIGMA_VISIBILITY RawTensorMetadata : RawTensorMetadataBase
{
    RawTensorMetadata()                                        = default;
    RawTensorMetadata(const RawTensorMetadata&)                = default;
    RawTensorMetadata(RawTensorMetadata&&) noexcept            = default;
    RawTensorMetadata& operator=(const RawTensorMetadata&)     = default;
    RawTensorMetadata& operator=(RawTensorMetadata&&) noexcept = default;
    ~RawTensorMetadata()                                       = default;
    explicit RawTensorMetadata(const xsigma::Tensor& t);

    // Wrap `weak_self_` in `std::optional` and split device into components to
    // keep struct default constructable. (which the std::array initializer needs)
    std::optional<WeakTensor>    weak_self_;
    xsigma::device_enum          device_type_{xsigma::device_enum::CPU};
    xsigma::device_option::int_t device_index_{-1};
};

// Used during post processing.
struct XSIGMA_VISIBILITY TensorMetadata : public RawTensorMetadataBase
{
    TensorMetadata(
        const RawTensorMetadata& r, std::vector<int64_t> sizes, std::vector<int64_t> strides);

    TensorImplAddress impl() const { return {}; }

    WeakTensor            weak_self_;
    xsigma::device_option device_;
    std::vector<int64_t>  sizes_;
    std::vector<int64_t>  strides_;

    // Set during `calculateUniqueTensorIDs`.
    std::optional<TensorID>     id_;
    std::optional<AllocationID> allocation_id_;
};

// Used during post processing.
struct XSIGMA_VISIBILITY ProfilerStepInfo
{
    int64_t  start_time_ns;  // start time of the profiler step
    int64_t  end_time_ns;    // end time of the profiler step
    uint64_t out_idx;        // index of the profiler step in the profiler "out" var in
                             // getRecords

    ProfilerStepInfo(int64_t start, int64_t end, uint64_t out_idx)
        : start_time_ns(start), end_time_ns(end), out_idx(out_idx)
    {
    }
};

using op_input_t =
    std::variant<TensorMetadata, std::vector<TensorMetadata>, xsigma::IValue, std::nullopt_t>;

// ============================================================================
// == ExtraFields =============================================================
// ============================================================================
template <EventType>
struct ExtraFields;

struct TorchOpBasicFields
{
    int64_t             sequence_number_{0};
    uint64_t            forward_tid_{0};
    xsigma::RecordScope scope_{};
    bool                is_async_{false};
    uint64_t            record_function_id_{0};
    int64_t             debug_handle_{0};
    std::string         name_;
    std::string         overload_name_;

    // Set in the exit callback.
    uint64_t end_tid_{0};
};

using jit_stack_t   = std::vector<std::string>;
using jit_modules_t = std::vector<std::string>;
using extra_args_t  = std::unordered_map<std::string, xsigma::IValue>;
using extra_meta_t  = std::unordered_map<std::string, std::string>;
using kwinputs_t    = std::unordered_map<std::string, xsigma::IValue>;

struct FallbackPair
{
    ProfilerVoidEventStub device_event_start_ = nullptr;
    ProfilerVoidEventStub device_event_end_   = nullptr;
};

template <>
struct ExtraFields<EventType::TorchOp> : TorchOpBasicFields
{
    ExtraFields(
        TorchOpBasicFields&&               f,
        uint64_t                           correlation_id,
        xsigma::time_t                     end_time_ns,
        std::vector<op_input_t>&&          inputs,
        std::vector<op_input_t>&&          concrete_inputs,
        jit_stack_t&&                      jit_stack,
        jit_modules_t&&                    jit_modules,
        extra_args_t&&                     extra_args,
        extra_meta_t&&                     extra_meta,
        kwinputs_t&&                       kwinputs,
        FallbackPair&&                     device_fallback,
        bool                               allow_tf32_cublas,
        std::unique_ptr<perf_counters_t>&& perf_event_counters)
        : TorchOpBasicFields(std::move(f)),
          correlation_id_{correlation_id},
          end_time_ns_{end_time_ns},
          inputs_{std::move(inputs)},
          concrete_inputs_{std::move(concrete_inputs)},
          jit_stack_{std::move(jit_stack)},
          jit_modules_{std::move(jit_modules)},
          extra_args_{std::move(extra_args)},
          extra_meta_{std::move(extra_meta)},
          kwinputs_{std::move(kwinputs)},
          device_fallback_{std::move(device_fallback)},
          allow_tf32_cublas_{allow_tf32_cublas},
          perf_event_counters_{std::move(perf_event_counters)}
    {
    }
    uint64_t                         correlation_id_;
    xsigma::time_t                   end_time_ns_;
    std::vector<op_input_t>          inputs_;
    std::vector<op_input_t>          concrete_inputs_;
    jit_stack_t                      jit_stack_;
    jit_modules_t                    jit_modules_;
    extra_args_t                     extra_args_;
    extra_meta_t                     extra_meta_;
    kwinputs_t                       kwinputs_;
    FallbackPair                     device_fallback_;
    bool                             allow_tf32_cublas_;
    std::unique_ptr<perf_counters_t> perf_event_counters_;
    std::string                      metadata_json_;
};

template <>
struct ExtraFields<EventType::Backend>
{
    int64_t             start_time_us_;
    int64_t             end_time_us_;
    int64_t             debug_handle_;
    xsigma::RecordScope scope_;
    std::string         name_;
    std::string         backend_;
    jit_stack_t         jit_stack_;
    jit_modules_t       jit_modules_;
};

template <>
struct ExtraFields<EventType::PythonGC>
{
    std::string phase;
    int64_t     duration_ns_;
};

template <>
struct ExtraFields<EventType::Vulkan>
{
    using raw_event_t = std::pair<xsigma::approx_time_t, vulkan_id_t>;
    std::string name_;
    int64_t     duration_ns_{0};
    // While building the event tree, we want to report a vulkan event's duration
    // as 0 so that its end time doesn't exceed that of its parent cpu op
    bool in_tree_building_{false};
};

struct RawAllocation
{
    xsigma::approx_time_t        start_time_;
    void*                        ptr_;
    int64_t                      alloc_size_;
    size_t                       total_allocated_;
    size_t                       total_reserved_;
    xsigma::device_enum          device_type_;
    xsigma::device_option::int_t device_index_;
};

// For performance.
static_assert(std::is_trivial_v<RawAllocation>, "Non-Trivial member of RawAllocation.");

template <>
struct ExtraFields<EventType::Allocation> : RawAllocation
{
    ExtraFields(const RawAllocation& allocation) : RawAllocation(allocation) {}

    xsigma::device_option device() const { return {device_type_, device_index_}; }

    std::optional<TensorID>     id_;
    std::optional<AllocationID> allocation_id_;
};

template <>
struct ExtraFields<EventType::OutOfMemory>
{
    xsigma::approx_time_t        start_time_;
    int64_t                      alloc_size_;
    size_t                       total_allocated_;
    size_t                       total_reserved_;
    xsigma::device_enum          device_type_;
    xsigma::device_option::int_t device_index_;
};

// For performance.
static_assert(
    std::is_trivial_v<ExtraFields<EventType::OutOfMemory>>,
    "Non-Trivial member of ExtraFields<EventType::OutOfMemory>.");

struct PyFrameState
{
    int                line_no_;
    xsigma::StringView filename_;
    xsigma::StringView funcname_;
};

template <typename T, typename Tag>
using strong_t = strong::type<T, Tag, strong::regular, strong::convertible_to<T>, strong::hashable>;
#if 0
using PyModuleSelf    = strong_t<PyObject*, struct PyModuleSelf_>;
using PyModuleCls     = strong_t<PyObject*, struct PyModuleCls_>;
using PyMethod        = strong_t</*PyMethodDef*/ void*, struct PyMethod_>;
using PyOptimizerSelf = strong_t<PyObject*, struct PyOptSelf_>;
using PyOptimizerCls  = strong_t<PyObject*, struct PyOptimizer_>;

struct NNModuleInfo
{
    struct ParameterInfo
    {
        std::string                   name_;
        TensorMetadata                metadata_;
        std::optional<TensorMetadata> grad_metadata_;
    };

    PyModuleSelf       self_;
    PyModuleCls        cls_;
    xsigma::StringView cls_name_;

    std::vector<ParameterInfo> parameters_;
    // Indicates that `self_` is the kth instance of `cls_` observed.
    size_t id_{std::numeric_limits<size_t>::max()};
};

struct OptimizerInfo
{
    struct ParameterInfo
    {
        TensorMetadata                                      metadata_;
        std::optional<TensorMetadata>                       grad_metadata_;
        std::vector<std::pair<std::string, TensorMetadata>> state_;
    };

    PyOptimizerSelf    self_;
    PyOptimizerCls     cls_;
    xsigma::StringView cls_name_;

    std::vector<ParameterInfo> parameters_;
};

struct PyExtraFieldsBase
{
    PyExtraFieldsBase(xsigma::time_t end_time_ns, size_t python_tid, PyFrameState caller)
        : end_time_ns_{end_time_ns}, python_tid_{python_tid}, caller_{std::move(caller)}
    {
    }

    xsigma::time_t end_time_ns_;
    size_t         python_tid_;
    PyFrameState   caller_;

    // kth python event observed. (Used by TensorBoard)
    size_t id_{std::numeric_limits<size_t>::max()};
};

template <>
struct ExtraFields<EventType::PyCall> : public PyExtraFieldsBase
{
    struct args_t
    {
        PyFrameState                 frame_state_;
        std::optional<NNModuleInfo>  module_info_;
        std::optional<OptimizerInfo> optimizer_info_;
    };

    ExtraFields(xsigma::time_t end_time_ns, size_t python_tid, PyFrameState caller, args_t args)
        : PyExtraFieldsBase(end_time_ns, python_tid, std::move(caller)),
          callsite_{std::move(args.frame_state_)},
          module_{std::move(args.module_info_)},
          optimizer_{std::move(args.optimizer_info_)}
    {
    }

    PyFrameState                 callsite_;
    std::optional<NNModuleInfo>  module_;
    std::optional<OptimizerInfo> optimizer_;
};

template <>
struct ExtraFields<EventType::PyCCall> : public PyExtraFieldsBase
{
    using args_t = xsigma::StringView;

    ExtraFields(xsigma::time_t end_time_ns, size_t python_tid, PyFrameState caller, args_t args)
        : PyExtraFieldsBase(end_time_ns, python_tid, std::move(caller)),
          function_name_{std::move(args)}
    {
    }

    xsigma::StringView function_name_;
};
#endif

template <>
struct ExtraFields<EventType::Kineto>
{
    // Mirrors `libkineto::GenericTraceActivity::Flow`. This information is used
    // during post processing to properly embed Kineto events into the broader
    // profiler tree structure. End users are not generally expected to use these
    // fields directly, but they are available for debugging.
    struct Flow
    {
        uint32_t id{0};
        uint32_t type{0};
        uint32_t start{0};
    };

    std::string             name_;
    int64_t                 duration_ns_{0};
    uint64_t                correlation_id_{0};
    libkineto::ActivityType activity_type_;
    Flow                    flow;
    std::weak_ptr<Result>   linked_activity_;
    std::string             metadata_json_;
};

struct XSIGMA_VISIBILITY Result : public std::enable_shared_from_this<Result>
{
    template <typename... Args>
    [[nodiscard]] static std::shared_ptr<Result> create(Args... args)
    {
        return std::shared_ptr<Result>(new Result(std::forward<Args>(args)...));
    }

    template <typename T>
    auto visit(T&& visitor)
    {
        return std::visit(std::forward<T>(visitor), extra_fields_);
    }

    template <typename T>
    auto visit(T&& visitor) const
    {
        return std::visit(std::forward<T>(visitor), extra_fields_);
    }

    template <typename T, typename Fn>
    void visit_if_base(const Fn& fn) const
    {
        visit(
            [&](const auto& extra_fields)
            {
                using extra_fields_t = typename std::remove_cv_t<
                    typename std::remove_reference_t<decltype(extra_fields)>>;

                if constexpr (std::is_base_of_v<T, extra_fields_t>)
                {
                    fn(extra_fields);
                }
            });
    }

    EventType tag() const
    {
        return visit([](const auto& i) { return deduceTag(i); });
    }

    std::string             name() const;
    std::string             overload_name() const;
    libkineto::ActivityType kinetoType() const;
    uint64_t                correlationID() const;
    int64_t                 endTimeNS() const;
    uint64_t                endTID() const;
    xsigma::device_enum     deviceType() const;

    int64_t                   start_time_ns_;
    uint64_t                  start_tid_;
    kineto::DeviceAndResource kineto_info_;
    std::variant<
        ExtraFields<EventType::TorchOp>,
        ExtraFields<EventType::Backend>,
        ExtraFields<EventType::Vulkan>,
        ExtraFields<EventType::Allocation>,
        ExtraFields<EventType::OutOfMemory>,
        ExtraFields<EventType::Kineto>/*,
        ExtraFields<EventType::PyCall>,
        ExtraFields<EventType::PyCCall>,
        ExtraFields<EventType::PythonGC>*/>
        extra_fields_;

    std::weak_ptr<Result>                             parent_;
    std::vector<std::shared_ptr<Result>>              children_;
    bool                                              finished_{false};
    bool                                              hidden_{false};
    const xsigma::profiler::impl::kineto::activity_t* kineto_activity_{nullptr};

private:
    template <EventType E>
    Result(
        int64_t                   start_time_ns,
        uint64_t                  start_tid,
        kineto::DeviceAndResource kineto_info,
        ExtraFields<E>&&          extra_fields)
        : start_time_ns_{start_time_ns},
          start_tid_{start_tid},
          kineto_info_{kineto_info},
          extra_fields_{std::move(extra_fields)}
    {
    }

    template <EventType E>
    static EventType deduceTag(const ExtraFields<E>& /*unused*/)
    {
        return E;
    }
};

struct KinetoObserverContext : public xsigma::ObserverContext
{
    struct Event
    {
        TorchOpBasicFields    basic_fields_;
        xsigma::approx_time_t start_time_;

        // Set in the exit callback.
        xsigma::approx_time_t end_time_{std::numeric_limits<xsigma::approx_time_t>::min()};

        bool                             allow_tf32_cublas_;
        std::unique_ptr<perf_counters_t> counters_;
        extra_meta_t*                    extra_nccl_meta_{};
    };

    explicit KinetoObserverContext(Event* event) : event_{event} {}

    Event*        event_;
    FallbackPair* fallback_{nullptr};
};

constexpr int IO_ENCODER_DEFAULT_BLOCK_SIZE = 1024;

constexpr int SCALAR_LIST_LENGTH_LIMIT = 30;

// InputOutputEncoder
// Stores each op_events' shapes and dtypes, and concrete values into a
// contiguous AppendOnlyList so that we no longer create vectors for shapes
// and dtypes on every op. Those vectors can be created during
// post-processing.
// It splits the data into two categories: input shapes and concrete inputs.
class InputOutputEncoder final
{
public:
    void push(xsigma::array_ref<const xsigma::IValue> values);

    // Used during post-processing to unpack the encoded data.
    // Each method returns a "supplier" lambda which takes no arguments;
    // invoking the lambda once will return a list of args that represent
    // the inputs for one op.
    // The data is split into two streams: "input shapes" and "concrete inputs".
    // Note: "auto" only works because these are only used in collection.cpp,
    // where they are implemented.
    auto getInputShapeGenerator();
    auto getConcreteInputGenerator();

    static bool isSupportedScalarList(const xsigma::IValue& list_candidate);

    void clear();

    enum class Tag
    {
        Tensor = 0,
        UndefinedTensor,
        TensorListBegin,  // TODO: generalize to other lists.
        ScalarList,
        Scalar,
        Other,
        TERMINATOR
    };

    enum class IOType
    {
        Shapes,
        ConcreteInputs,
        None
    };

private:
    void push(const xsigma::Tensor& t);

    // Implementation detail for getInputShapeGenerator and
    // getConcreteInputGenerator
    auto getIValueGenerator(const IOType& io_type);

    AppendOnlyList<Tag, IO_ENCODER_DEFAULT_BLOCK_SIZE>               tags_;
    AppendOnlyList<RawTensorMetadata, IO_ENCODER_DEFAULT_BLOCK_SIZE> tensor_metadata_;
    AppendOnlyList<int64_t, IO_ENCODER_DEFAULT_BLOCK_SIZE>           tensor_sizes_strides_;
    AppendOnlyList<xsigma::IValue, IO_ENCODER_DEFAULT_BLOCK_SIZE>    ivalues_;
};

using perf_profiler_t = xsigma::profiler::impl::linux_perf::PerfProfiler;

class XSIGMA_VISIBILITY ThreadLocalSubqueue
{
public:
    ThreadLocalSubqueue(const uint64_t tid, ProfilerConfig config);

    std::unique_ptr<KinetoObserverContext> begin_op(const xsigma::RecordFunction& fn);

    template <class... Args>
    void emplace_backend_event(Args&&... args)
    {
        backend_events_.emplace_back(std::forward<Args>(args)...);
    }

    template <class... Args>
    void emplace_vulkan_event(Args&&... args)
    {
        vulkan_events_.emplace_back(std::forward<Args>(args)...);
    }

    template <class... Args>
    void emplace_allocation_event(Args&&... args)
    {
        allocations_.emplace_back(std::forward<Args>(args)...);
    }

    template <class... Args>
    void emplace_ooms_event(Args&&... args)
    {
        ooms_.emplace_back(std::forward<Args>(args)...);
    }

    template <class... Args>
    void emplace_py_call(Args&&... args)
    {
        py_calls_.emplace_back(std::forward<Args>(args)...);
    }

    template <class... Args>
    void emplace_gc_call(Args&&... args)
    {
        pythongc_.emplace_back(std::forward<Args>(args)...);
    }

    uint64_t tid() const { return tid_; }

    const kineto::DeviceAndResource& kineto_info() const { return kineto_info_; }

    inline void disable_perf_profiler(perf_counters_t& counters) const
    {
        perf_profiler_->Disable(counters);
    }

private:
    uint64_t                         tid_;
    ProfilerConfig                   config_;
    kineto::DeviceAndResource        kineto_info_;
    std::unique_ptr<perf_profiler_t> perf_profiler_;

    friend class RecordQueue;
    // See `containers.h` for block size benchmarks.
    static constexpr size_t BlockSize = 512;

    struct TorchOpStorage
    {
        // NB: This is a destructive operation.
        void materialize(
            std::vector<std::shared_ptr<Result>>&                       out,
            std::vector<ProfilerStepInfo>&                              step_info,
            const std::function<xsigma::time_t(xsigma::approx_time_t)>& time_converter,
            const uint64_t                                              tid,
            const kineto::DeviceAndResource&                            kineto_info);

        template <typename T, size_t ChunkSize>
        class EventBlock : public std::array<T, ChunkSize>
        {
        public:
            EventBlock();
            uint64_t correlation_id(const T* ptr) const;

        private:
            uint64_t id_start_;
        };

        using event_t = KinetoObserverContext::Event;
        class OpList : public AppendOnlyList<event_t, BlockSize, EventBlock>
        {
        public:
            template <class... Args>
            std::pair<event_t*, uint64_t> emplace_back(Args&&... args);
            static uint64_t               correlationID(const OpList::Iterator& e);
        } op_events_;

        // report_input_shapes
        InputOutputEncoder inputs_outputs_;

        // with_stack (JIT)
        AppendOnlyList<jit_stack_t, BlockSize> jit_stack_;

        // with_modules
        AppendOnlyList<jit_modules_t, BlockSize> jit_modules_;

        // with_flops
        AppendOnlyList<extra_args_t, BlockSize> extra_args_;

        // report extra metadata, i.e. collective communication meta
        AppendOnlyList<extra_meta_t, BlockSize> extra_meta_;

        // report kwinputs
        AppendOnlyList<kwinputs_t, BlockSize> kwinputs_;

        // ProfilerState::KINETO_GPU_FALLBACK or
        // ProfilerState::KINETO_PRIVATEUSE1_FALLBACK
        AppendOnlyList<FallbackPair, BlockSize> device_fallback_;
    } torch_ops_;

    // reportBackendEventToActiveKinetoProfiler
    AppendOnlyList<ExtraFields<EventType::Backend>, BlockSize> backend_events_;

    // _reportVulkanEventToProfiler
    AppendOnlyList<ExtraFields<EventType::Vulkan>::raw_event_t, BlockSize> vulkan_events_;

    // reportMemoryUsage
    AppendOnlyList<RawAllocation, BlockSize> allocations_;

    // reportOOMs
    AppendOnlyList<ExtraFields<EventType::OutOfMemory>, BlockSize> ooms_;

    // with_stack (Python)
    AppendOnlyList<std::pair<python_tracer::TraceKey, xsigma::approx_time_t>, BlockSize> py_calls_;
    // gc with_stack (Python)
    AppendOnlyList<std::pair<std::string, xsigma::approx_time_t>, BlockSize> pythongc_;
};

class XSIGMA_VISIBILITY RecordQueue
{
public:
    RecordQueue(ProfilerConfig config, std::set<ActivityType> activities);

    bool                 tracePython() const;
    bool                 getPythonGcEvents() const;
    ThreadLocalSubqueue* getSubqueue();
    void                 stop();
    void                 restart();

    // NB: This is a destructive operation.
    std::pair<
        std::vector<std::shared_ptr<Result>>,
        std::unique_ptr<xsigma::profiler::impl::kineto::ActivityTraceWrapper>>
    getRecords(
        std::function<xsigma::time_t(xsigma::approx_time_t)> time_converter,
        uint64_t                                             start_time_ns,
        uint64_t                                             end_time_ns);

private:
    uint32_t                                                              id_;
    ProfilerConfig                                                        config_;
    std::set<ActivityType>                                                activities_;
    xsigma::flat_hash_map<uint64_t, std::unique_ptr<ThreadLocalSubqueue>> sub_queues_;
    std::mutex                                                            sub_queue_mutex_;
    std::unique_ptr<python_tracer::PythonTracerBase>                      python_tracer_;
};

XSIGMA_API bool get_record_concrete_inputs_enabled();
XSIGMA_API void set_record_concrete_inputs_enabled_fn(std::function<bool()> /*fn*/);
XSIGMA_API void set_record_concrete_inputs_enabled_val(bool /*val*/);

XSIGMA_API bool get_fwd_bwd_enabled();
XSIGMA_API void set_fwd_bwd_enabled_fn(std::function<bool()> /*fn*/);
XSIGMA_API void set_fwd_bwd_enabled_val(bool /*val*/);

XSIGMA_API bool get_cuda_sync_enabled();
XSIGMA_API void set_cuda_sync_enabled_fn(std::function<bool()> /*fn*/);
XSIGMA_API void set_cuda_sync_enabled_val(bool /*val*/);

// Comms related RecordFunctions will record information about tensor storage
// locations.
XSIGMA_API bool get_record_tensor_addrs_enabled();
XSIGMA_API void set_record_tensor_addrs_enabled_fn(std::function<bool()> /*fn*/);
XSIGMA_API void set_record_tensor_addrs_enabled_val(bool /*val*/);

}  // namespace xsigma::profiler::impl
