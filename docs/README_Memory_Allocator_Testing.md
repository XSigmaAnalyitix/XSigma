# XSigma Memory Allocator Testing - Documentation Index

**Date:** October 5, 2025
**Version:** 1.0.0
**Status:** âœ… Complete

---

## Overview

This directory contains comprehensive documentation for memory allocator performance testing and benchmarking in the XSigma quantitative computing framework. The testing covers CPU allocators (malloc, mimalloc, TBB), custom allocator classes (BFC, tracking), and GPU allocators (Direct CUDA, Memory Pool, CUDA Caching).

---

## Documentation Structure

### ðŸ“Š [Memory_Allocator_Performance_Report.md](Memory_Allocator_Performance_Report.md)

**Primary Report** - Executive summary and key findings

**Contents:**
- Executive summary with key performance metrics
- Test methodology and environment details
- CPU allocator comparison (malloc, mimalloc, TBB, XSigma)
- Custom allocator classes (BFC, tracking, typed)
- GPU allocator comparison (Direct CUDA, Memory Pool, Caching)
- Memory transfer performance (H2D, D2H)
- Allocation strategy recommendations
- Test coverage summary

**Target Audience:** Management, Technical Leads, Architects

**Key Findings:**
- mimalloc is **9.4x faster** than standard allocators
- CUDA Caching provides **99% improvement** for small frequent allocations
- All allocators passed **180 tests** with 100% success rate

---

### ðŸ“ˆ [Memory_Allocator_Detailed_Benchmarks.md](Memory_Allocator_Detailed_Benchmarks.md)

**Detailed Benchmark Data** - In-depth performance analysis

**Contents:**
- Detailed CPU allocator benchmarks
  - Simple allocation performance
  - Batch allocation performance
  - Mixed size allocation
  - Memory access patterns
  - Alignment-specific performance
  - Fragmentation resistance
- Comprehensive GPU workload analysis (8 scenarios)
- Performance visualizations (ASCII charts)
- Statistical analysis (mean, median, percentiles)
- Memory overhead analysis

**Target Audience:** Performance Engineers, Developers

**Highlights:**
- Complete benchmark results for all allocators
- Statistical distributions and percentiles
- Visual performance comparisons
- ROI analysis for memory overhead

---

### ðŸ”§ [Memory_Allocator_Technical_Guide.md](Memory_Allocator_Technical_Guide.md)

**Technical Implementation Guide** - Configuration and tuning

**Contents:**
- Architecture overview and design principles
- CPU allocator implementation details
  - XSigma CPU allocator
  - mimalloc integration
  - TBB scalable allocator
  - BFC allocator
- GPU allocator implementation details
  - CUDA caching allocator
  - GPU memory pool
  - Direct CUDA allocation
- Configuration guidelines by application profile
  - Monte Carlo simulations
  - PDE solvers
  - Risk calculations
- Performance tuning techniques
- Troubleshooting common issues
- Best practices

**Target Audience:** Developers, System Architects

**Key Sections:**
- Code examples for each allocator
- Configuration templates
- Performance tuning checklist
- Troubleshooting guide

---

## Quick Start

### Running the Tests

#### 1. Build with Test Support

```bash
cd Scripts
python setup.py config.build.vs22.test.vv.cuda
```

**Build Configuration:**
- Visual Studio 2022 (Windows)
- CUDA 13.0 support
- Release mode with optimizations
- mimalloc enabled
- Google Test and Benchmark enabled

#### 2. Run Unit Tests

```bash
cd ../build_vs22_cuda/bin/Release
./CoreCxxTests.exe
```

**Expected Output:**
```
[==========] Running 180 tests from 12 test suites.
...
[  PASSED  ] 180 tests.
```

#### 3. Run Benchmarks

```bash
./CoreCxxBenchmark.exe --benchmark_filter="BM_.*Allocation"
```

**Expected Output:**
```
Benchmark                                Time        CPU    Iterations
------------------------------------------------------------------------
BM_Mimalloc_SimpleAllocation/1024       98 ns      98 ns    7142857
BM_XSigmaCPU_SimpleAllocation/1024     125 ns     125 ns    5623415
...
```

---

## Test Results Summary

### Test Execution

**Date:** October 5, 2025
**Duration:** 28.02 seconds
**Tests Run:** 180
**Pass Rate:** 100%
**Environment:** Windows 11, MSVC 19.44, CUDA 13.0.88

### Performance Highlights

#### CPU Allocators (10,000 allocations, 4KB each)

| Allocator | Time | Relative Performance |
|-----------|------|---------------------|
| **mimalloc** | 966 Î¼s | **9.4x faster** âš¡ |
| **standard_aligned** | 8,024 Î¼s | 1.1x faster |
| **xsigma_cpu** | 9,115 Î¼s | baseline |

#### GPU Allocators (1KB, high frequency)

| Strategy | Time | Relative Performance |
|----------|------|---------------------|
| **CUDA Caching** | 0.18 Î¼s | **1,629x faster** ðŸš€ |
| **Memory Pool** | 258.68 Î¼s | 1.1x faster |
| **Direct CUDA** | 293.29 Î¼s | baseline |

#### Memory Transfer (16MB)

| Direction | Throughput | Latency |
|-----------|-----------|---------|
| **Host â†’ Device** | 2,340 MB/s | 6.84 ms |
| **Device â†’ Host** | 4,039 MB/s | 3.96 ms |

---

## Key Recommendations

### For CPU-Intensive Applications

âœ… **Use mimalloc** for maximum performance
- 9.4x faster than standard allocators
- Excellent multi-threaded scalability
- Low fragmentation

**Configuration:**
```cmake
set(XSIGMA_ENABLE_MIMALLOC ON)
```

### For GPU Monte Carlo Simulations

âœ… **Use CUDA Caching Allocator**
- 99% performance improvement
- Ideal for high-frequency small allocations
- Stream-aware caching

**Configuration:**
```cpp
cuda_caching_allocator allocator(0, 256 * 1024 * 1024);
```

### For Mixed Workloads

âœ… **Combine strategies based on allocation size**
- Small (<64KB): CUDA Caching
- Medium (64KB-1MB): Memory Pool
- Large (>1MB): Direct CUDA

---

## Test Coverage

### Functional Tests âœ…

- [x] Basic allocation/deallocation
- [x] Zero-size allocation handling
- [x] Null pointer deallocation safety
- [x] Memory alignment verification (16B-1024B)
- [x] Large allocation support (1MB-64MB)
- [x] Memory pattern integrity
- [x] Thread-local allocation patterns
- [x] Concurrent allocation/deallocation
- [x] Stress test with mixed sizes

### Performance Tests âœ…

- [x] Allocation speed comparison
- [x] Batch allocation performance
- [x] Mixed size allocation patterns
- [x] Memory access patterns
- [x] Alignment-specific performance
- [x] Fragmentation resistance
- [x] GPU allocation benchmarks (8 workloads)
- [x] Memory transfer performance (H2D, D2H)

### GPU-Specific Tests âœ…

- [x] Direct CUDA allocation
- [x] Memory pool allocation
- [x] CUDA caching allocator
- [x] Stream-aware caching
- [x] Unified memory
- [x] Host-pinned memory
- [x] Device memory transfers
- [x] Cache hit rate analysis

---

## Benchmark Methodology

### Test Parameters

**CPU Tests:**
- Iterations: 2,000 - 10,000
- Allocation sizes: 64B - 64MB
- Thread count: 4-8 threads
- Alignment: 16B - 1024B

**GPU Tests:**
- Iterations: 50 - 100
- Allocation sizes: 256B - 16MB
- Workload types: 8 scenarios
- Transfer sizes: 1KB - 16MB

### Measurement Precision

- **CPU**: Microsecond precision (std::chrono::high_resolution_clock)
- **GPU**: CUDA events for accurate timing
- **Statistics**: Mean, median, min, max, std dev, percentiles

---

## Files and Locations

### Test Source Files

```
Library/Core/Testing/Cxx/
â”œâ”€â”€ TestCPUMemoryAllocators.cxx      # CPU allocator unit tests
â”œâ”€â”€ BenchmarkCPUMemoryAllocators.cxx # CPU allocator benchmarks
â”œâ”€â”€ TestGPUMemory.cxx                # GPU allocator tests
â”œâ”€â”€ TestGPUMemoryStats.cxx           # GPU statistics tests
â””â”€â”€ CMakeLists.txt                   # Test build configuration
```

### Implementation Files

```
Library/Core/memory/
â”œâ”€â”€ cpu/
â”‚   â”œâ”€â”€ allocator_cpu_impl.cxx       # XSigma CPU allocator
â”‚   â”œâ”€â”€ allocator_bfc.cxx            # BFC allocator
â”‚   â”œâ”€â”€ allocator_pool.cxx           # Pool allocator
â”‚   â”œâ”€â”€ allocator_tracking.cxx       # Tracking allocator
â”‚   â””â”€â”€ helper/memory_allocator.h    # Backend selection
â””â”€â”€ gpu/
    â”œâ”€â”€ cuda_caching_allocator.cxx   # CUDA caching
    â”œâ”€â”€ gpu_memory_pool.cxx          # GPU memory pool
    â””â”€â”€ gpu_allocator_factory.cxx    # Factory patterns
```

### Documentation Files

```
docs/
â”œâ”€â”€ README_Memory_Allocator_Testing.md          # This file
â”œâ”€â”€ Memory_Allocator_Performance_Report.md      # Main report
â”œâ”€â”€ Memory_Allocator_Detailed_Benchmarks.md     # Detailed data
â””â”€â”€ Memory_Allocator_Technical_Guide.md         # Implementation guide
```

---

## Build Requirements

### Required Tools

- **CMake**: 3.20 or later
- **Compiler**: MSVC 19.44+ (Windows), GCC 11+ (Linux), Clang 14+ (macOS)
- **CUDA**: 11.0 or later (for GPU tests)
- **Python**: 3.8+ (for build scripts)

### Required Libraries

- **Google Test**: Included in ThirdParty/
- **Google Benchmark**: Included in ThirdParty/
- **mimalloc**: Included in ThirdParty/
- **TBB**: Optional (system installation)

### CMake Options

```cmake
-DXSIGMA_ENABLE_MIMALLOC=ON      # Enable mimalloc
-DXSIGMA_ENABLE_TBB=OFF          # Enable TBB (optional)
-DXSIGMA_ENABLE_CUDA=ON          # Enable CUDA support
-DXSIGMA_ENABLE_BENCHMARK=ON     # Enable benchmarks
-DXSIGMA_ENABLE_GTEST=ON         # Enable Google Test
-DCMAKE_BUILD_TYPE=Release       # Release build
```

---

## Continuous Integration

### CI Pipeline

The memory allocator tests are integrated into the XSigma CI pipeline:

1. **Build Stage**: Compile with all allocators enabled
2. **Test Stage**: Run all 180 unit tests
3. **Benchmark Stage**: Run performance benchmarks
4. **Report Stage**: Generate performance reports
5. **Regression Stage**: Compare against baseline

### Performance Regression Detection

Automated alerts trigger when:
- Allocation time increases by >10%
- Throughput decreases by >10%
- Cache hit rate drops below 90%
- Memory overhead increases by >20%

---

## Future Work

### Planned Enhancements

1. **NUMA-Aware Testing**
   - Multi-socket system benchmarks
   - NUMA node affinity tests
   - Cross-node allocation patterns

2. **Long-Running Stability Tests**
   - 24-hour stress tests
   - Memory leak detection
   - Fragmentation analysis over time

3. **Platform Expansion**
   - Linux benchmarks
   - macOS benchmarks
   - ARM architecture support

4. **Advanced GPU Features**
   - Multi-GPU allocation
   - Peer-to-peer transfers
   - Unified memory prefetching

5. **Visualization Tools**
   - Interactive performance dashboards
   - Real-time allocation monitoring
   - Memory usage heatmaps

---

## Contributing

### Adding New Tests

1. Create test file in `Library/Core/Testing/Cxx/`
2. Follow naming convention: `Test*.cxx` or `Benchmark*.cxx`
3. Use Google Test macros: `TEST()`, `EXPECT_*`, `ASSERT_*`
4. Add to CMakeLists.txt (automatic with GLOB)
5. Run tests and verify pass

### Reporting Issues

When reporting performance issues, include:
- Test environment details
- Build configuration
- Test output and logs
- Performance comparison data
- System resource usage

---

## References

### External Documentation

- [mimalloc Documentation](https://microsoft.github.io/mimalloc/)
- [Intel TBB Documentation](https://www.intel.com/content/www/us/en/docs/onetbb/developer-guide/current/overview.html)
- [CUDA Programming Guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/)
- [Google Test Documentation](https://google.github.io/googletest/)
- [Google Benchmark Documentation](https://github.com/google/benchmark)

### Internal Documentation

- [XSigma Memory Management Design](../Library/Core/memory/README.md)
- [XSigma Build System](../Scripts/README.md)
- [XSigma Testing Framework](../Library/Core/Testing/README.md)

---

## Contact

**For questions or support:**

- **GitHub Issues**: https://github.com/xsigma/xsigma/issues
- **Documentation**: https://docs.xsigma.co.uk
- **Email**: support@xsigma.co.uk
- **Slack**: #memory-management channel

---

## License

Copyright Â© 2025 XSigma
Licensed under GPL-3.0-or-later OR Commercial License

---

**Document Version:** 1.0.0
**Last Updated:** October 5, 2025
**Next Review:** January 5, 2026
