Executive Summary

Modular design with clear separation: core thread pool, high-level parallel API, specialized 1D work-stealing, and optional OpenMP/TBB backends.
Strengths: good docs/tests, NUMA hooks, exception propagation, lazy pool init, backend info surfaces.
Key issues to address:
parallel_for/parallel_reduce use the global inter-op pool and a global barrier; can deadlock/block unrelated tasks and ignores backend selection.
intraop_launch checks the wrong pool and doesn’t properly handle nested TLS.
Data race on thread_pool’s exception_ and unused state/params.
parallelize_1d creates a fresh pool per call (heavy), ignores configured thread counts, misuse/unused completion counters.
Backend selection isn’t respected by core parallel_for/parallel_reduce.
Structure

Core pool: Library/Core/smp_new/core/thread_pool.{h,cxx}
High-level API: Library/Core/smp_new/parallel/parallel_api.{h,hxx,cxx}
1D work-stealing: Library/Core/smp_new/parallel/parallelize_1d.{h,cxx}
OpenMP backend: Library/Core/smp_new/openmp/parallel_openmp.{h,cxx}
TBB backend: Library/Core/smp_new/tbb/parallel_tbb.{h,hxx,cxx}
Core Thread Pool

API and behavior: ThreadPool exposes Run, WaitWorkComplete, Size, NumAvailable, InThreadPool with exception propagation.
Unused parameter: constructor docs mention per-thread init callback, but it’s never invoked: Library/Core/smp_new/core/thread_pool.h (line 145) and Library/Core/smp_new/core/thread_pool.cxx (line 39).
Data race on exception_: set in worker without lock, read in WaitWorkComplete under lock: Library/Core/smp_new/core/thread_pool.cxx (line 147) and Library/Core/smp_new/core/thread_pool.cxx (line 101). Protect with the same mutex or a dedicated mutex.
Dead/unused members: complete_ always set/never read: Library/Core/smp_new/core/thread_pool.h (line 125). Consider removal or adding a getter.
Global barrier semantics: WaitWorkComplete waits for all pool tasks; unsuitable when the pool is shared by multiple clients.
Parallel API

Templates currently hardwire to inter-op pool:
Grain sizing uses inter-op pool size: Library/Core/smp_new/parallel/parallel_api.hxx (line 33).
Work submission uses inter-op pool: Library/Core/smp_new/parallel/parallel_api.hxx (line 56) and Library/Core/smp_new/parallel/parallel_api.hxx (line 134).
Result: parallel_for/reduce block on unrelated inter-op tasks due to global barrier usage.
TLS for “in parallel region” not restored on exit (overwrites nested state):
Set/unset without restoring previous value: Library/Core/smp_new/parallel/parallel_api.hxx (line 40) and Library/Core/smp_new/parallel/parallel_api.hxx (line 51); similar in reduce: Library/Core/smp_new/parallel/parallel_api.hxx (line 114) and Library/Core/smp_new/parallel/parallel_api.hxx (line 125).
intraop_launch bug:
Checks inter-op pool availability instead of intra-op pool: Library/Core/smp_new/parallel/parallel_api.cxx (line 68).
TLS g_in_intraop_region is set on caller thread only, not in the worker thread that runs fn.
Backend selection surfaces but not honored by templates:
set_backend() initializes OpenMP/TBB, but parallel_for/reduce always use native pool and never call TBB/OpenMP paths.
parallelize_1d (Work-Stealing)

Coordinator initializes deques with DefaultNumThreads (ignores configured intra/inter op counts): Library/Core/smp_new/parallel/parallelize_1d.cxx (line 72).
Spawns a brand-new pool per call (heavy thread creation/teardown, adds latency): Library/Core/smp_new/parallel/parallelize_1d.cxx (line 110).
pending_work_ is incremented only for initial chunks; when stealing splits work, new chunks are enqueued without incrementing the counter: Library/Core/smp_new/parallel/parallelize_1d.cxx (line 195) to Library/Core/smp_new/parallel/parallelize_1d.cxx (line 201). Also, coordinator doesn’t use wait_complete(); relies on pool-wide barrier, so pending_work_ isn’t authoritative.
Deque ops are fully locked (owner pop and steal both lock), which is simple but reduces throughput for fine-grained tasks.
Flags param is unused: Library/Core/smp_new/parallel/parallelize_1d.h (line 103) and Library/Core/smp_new/parallel/parallelize_1d.cxx (line 229).
OpenMP Backend

Clean capability detection and controls (_OPENMP, MKL integration).
Functions behave reasonably: init, shutdown, set/get thread counts, info.
Not integrated into parallel_for/reduce routing.
TBB Backend

Uses XSIGMA_HAS_TBB for compile-time gating. CMake sets it: Cmake/tools/dependencies.cmake (line 44).
Exposes ParallelForTBB and templated ParallelReduceTBB with proper nested region restoration: Library/Core/smp_new/tbb/parallel_tbb.cxx (line 174) and Library/Core/smp_new/tbb/parallel_tbb.hxx (line 24).
Not integrated with parallel_for/reduce backend routing.
Concurrency & Correctness Issues

exception_ data race in thread pool: Library/Core/smp_new/core/thread_pool.cxx (line 147) vs Library/Core/smp_new/core/thread_pool.cxx (line 101).
Global barrier misuse: parallel_for/reduce calling pool.WaitWorkComplete() blocks unrelated inter-op tasks; consider local barrier per call: Library/Core/smp_new/parallel/parallel_api.hxx (line 85) and Library/Core/smp_new/parallel/parallel_api.hxx (line 161).
TLS region flag not restored, can incorrectly flip outer scope: Library/Core/smp_new/parallel/parallel_api.hxx (line 40) and Library/Core/smp_new/parallel/parallel_api.hxx (line 114).
intraop_launch uses wrong pool for capacity check: Library/Core/smp_new/parallel/parallel_api.cxx (line 68).
parallelize_1d completion counters not aligned with dynamically created stolen work: Library/Core/smp_new/parallel/parallelize_1d.cxx (line 195).
Performance Considerations

Pool creation per parallelize_1d() is expensive: Library/Core/smp_new/parallel/parallelize_1d.cxx (line 110). Reuse a long-lived pool (intra-op) or a dedicated worker group.
WorkStealingDeque locking limits throughput for tiny work items. Consider owner lock-free LIFO + locked stealing or chunking.
parallel_for grain sizing as n/(threads*4) is simplistic: Library/Core/smp_new/parallel/parallel_api.hxx (line 33). Consider dynamic chunking or min-chunk thresholds.
Size()/NumAvailable() take the pool mutex: Library/Core/smp_new/core/thread_pool.cxx (line 62) and Library/Core/smp_new/core/thread_pool.cxx (line 68). If these are hot, consider atomics.
API Consistency

parallel_api.h includes smp/xsigma_thread_local.h but the current templates do not use it: Library/Core/smp_new/parallel/parallel_api.h (line 7).
Backend selection is decoupled from the templates; end-users might expect set_backend(TBB) to steer parallel_for/reduce to TBB/OpenMP.
Documentation & Tests

Tests cover basic functionality, performance, nesting, and backend info:
Library/Core/Testing/Cxx/TestSmpNewParallelFor.cxx (line 1)
Library/Core/Testing/Cxx/TestSmpNewParallelReduce.cxx (line 14)
Library/Core/Testing/Cxx/TestSmpNewBackend.cxx (line 108)
Library/Core/Testing/Cxx/test_parallelize_1d.cxx (line 7)
Docs are thorough; consider documenting inter-op vs intra-op distinctions and backend routing behavior.
Priority Recommendations

Correctness
Protect exception_ with a mutex or write under mutex_: Library/Core/smp_new/core/thread_pool.cxx (line 147).
Fix intraop_launch to check intra-op pool availability: Library/Core/smp_new/parallel/parallel_api.cxx (line 68).
Restore TLS flags on exit (save/restore boolean): Library/Core/smp_new/parallel/parallel_api.hxx (line 40) and Library/Core/smp_new/parallel/parallel_api.hxx (line 114).
API/Design
Route parallel_for/reduce to the intra-op pool (not inter-op) and stop using pool-global barrier. Use a per-call local barrier (atomic counter + condition_variable) to avoid interference: Library/Core/smp_new/parallel/parallel_api.hxx (line 56) and Library/Core/smp_new/parallel/parallel_api.hxx (line 85).
Honor backend selection: if TBB selected call tbb::ParallelForTBB/ParallelReduceTBB; if OpenMP selected, add OpenMP-based implementations.
Performance
Reuse threads for parallelize_1d() (use intra-op pool workers running a coordinator). Avoid per-call pool creation: Library/Core/smp_new/parallel/parallelize_1d.cxx (line 110).
Optional: make owner-side deque pop lock-free or reduce lock contention with chunk sizes for tiny items.
Cleanup
Invoke init_thread callback or remove from API: Library/Core/smp_new/core/thread_pool.cxx (line 39).
Remove/repurpose complete_, num_tasks, results_mutex, unused includes: Library/Core/smp_new/core/thread_pool.h (line 125), Library/Core/smp_new/parallel/parallel_api.hxx (line 57), Library/Core/smp_new/parallel/parallel_api.hxx (line 131), Library/Core/smp_new/parallel/parallel_api.h (line 7).
Update parallelize_1d to either use pending_work_ correctly (bump on split) and a true completion wait, or remove the unused completion tracking.
Concrete Fix Pointers

ThreadPool exception race
Wrap exception_ writes with mutex_ or add exception_mutex_: Library/Core/smp_new/core/thread_pool.cxx (line 147).
intraop_launch availability check
Replace internal::GetInteropPool().NumAvailable() with GetIntraopPool().NumAvailable(): Library/Core/smp_new/parallel/parallel_api.cxx (line 68).
TLS restoration
Save previous g_in_parallel_region and restore on exit in both parallel_for/reduce: Library/Core/smp_new/parallel/parallel_api.hxx (line 40), Library/Core/smp_new/parallel/parallel_api.hxx (line 114).
Use intra-op pool for templates
Replace internal::GetInteropPool() with GetIntraopPool() and adopt a local barrier instead of pool.WaitWorkComplete(): Library/Core/smp_new/parallel/parallel_api.hxx (line 56) and Library/Core/smp_new/parallel/parallel_api.hxx (line 85).
Honor backend selection
In the template wrappers, branch on native::GetCurrentBackend() and call tbb::ParallelForTBB/ParallelReduceTBB when TBB is active: Library/Core/smp_new/parallel/parallel_api.hxx (line 20) and Library/Core/smp_new/tbb/parallel_tbb.h (line 89).
parallelize_1d pooling and counters
Reuse intra-op pool workers and move the coordinator’s inner loops into tasks; or create a singleton worker group. If keeping current shape, either increment pending_work_ for split work (Library/Core/smp_new/parallel/parallelize_1d.cxx (line 195)) and wait via wait_complete(), or remove completion CV entirely.
Nice-to-Haves

Validate inputs in set_num_{intra,interop}_threads (must be > 0): Library/Core/smp_new/parallel/parallel_api.cxx (line 89) and Library/Core/smp_new/parallel/parallel_api.cxx (line 112).
Consider atomics for available_ if reads are hot; keep lock for tasks_.
Expand get_parallel_info() to report actual pool sizes, backend, and pool availability: Library/Core/smp_new/parallel/parallel_api.cxx (line 168).
If you want, I can implement the critical fixes (exception_ locking, intraop_launch bug, TLS restoration, and switching parallel_for/reduce to a per-call barrier on the intra-op pool) and wire backend routing stubs to TBB with guarded calls.